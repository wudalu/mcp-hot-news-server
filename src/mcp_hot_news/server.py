#!/usr/bin/env python3
"""
åŸºäºfastmcpçš„ç°ä»£åŒ–çƒ­ç‚¹æ–°é—»MCPæœåŠ¡å™¨
æ”¯æŒå¤šå¹³å°çƒ­ç‚¹æ–°é—»èšåˆã€ç¼“å­˜ç®¡ç†å’Œè¶‹åŠ¿åˆ†æ
æ”¯æŒå›½å†…å¹³å°ï¼ˆvvhan APIï¼‰å’Œå…¨çƒå¹³å°ï¼ˆGoogle Trendsã€NewsAPIã€Redditã€Twitterç­‰ï¼‰
"""

import asyncio
import json
import logging
import os
import base64
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
import httpx
from fastmcp import FastMCP
from pydantic import BaseModel, Field

# å¯¼å…¥é…ç½®ç®¡ç†
from .config import mcp_config

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ================== æ•°æ®æ¨¡å‹ ==================


class NewsItem(BaseModel):
    """æ–°é—»æ¡ç›®æ•°æ®æ¨¡å‹"""

    title: str = Field(description="æ–°é—»æ ‡é¢˜")
    url: str = Field(description="æ–°é—»é“¾æ¥")
    hot_value: Optional[Union[str, int]] = Field(default=None, description="çƒ­åº¦å€¼")
    rank: Optional[int] = Field(default=None, description="æ’å")
    platform: str = Field(description="å¹³å°åç§°")
    timestamp: str = Field(description="è·å–æ—¶é—´")
    description: Optional[str] = Field(default="", description="æ–°é—»æè¿°")
    source: Optional[str] = Field(default="", description="æ–°é—»æ¥æº")
    controversy_score: Optional[float] = Field(default=0.0, description="äº‰è®®æ€§åˆ†æ•°")
    engagement_potential: Optional[float] = Field(default=0.0, description="äº’åŠ¨æ½œåŠ›")


class PlatformNews(BaseModel):
    """å¹³å°æ–°é—»æ•°æ®æ¨¡å‹"""

    platform: str = Field(description="å¹³å°åç§°")
    news_list: List[NewsItem] = Field(description="æ–°é—»åˆ—è¡¨")
    update_time: str = Field(description="æ›´æ–°æ—¶é—´")
    total_count: int = Field(description="æ–°é—»æ€»æ•°")
    platform_type: str = Field(default="domestic", description="å¹³å°ç±»å‹ï¼šdomestic/global")


class TrendAnalysis(BaseModel):
    """è¶‹åŠ¿åˆ†ææ•°æ®æ¨¡å‹"""

    hot_keywords: List[str] = Field(description="çƒ­é—¨å…³é”®è¯")
    trending_topics: List[str] = Field(description="è¶‹åŠ¿è¯é¢˜")
    platform_summary: Dict[str, int] = Field(description="å„å¹³å°çƒ­ç‚¹æ•°é‡")
    analysis_time: str = Field(description="åˆ†ææ—¶é—´")
    controversy_analysis: Dict[str, Any] = Field(default_factory=dict, description="äº‰è®®æ€§åˆ†æ")


# ================== ç¼“å­˜ç®¡ç†å™¨ ==================


@dataclass
class CacheItem:
    """ç¼“å­˜æ¡ç›®"""

    data: Any
    timestamp: datetime
    ttl: int = 3600  # 1å°æ—¶TTL


class CacheManager:
    """æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self):
        self._cache: Dict[str, CacheItem] = {}

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if key not in self._cache:
            return None

        item = self._cache[key]
        if datetime.now() - item.timestamp > timedelta(seconds=item.ttl):
            del self._cache[key]
            return None

        return item.data

    def set(self, key: str, data: Any, ttl: int = 3600) -> None:
        """è®¾ç½®ç¼“å­˜æ•°æ®"""
        self._cache[key] = CacheItem(data=data, timestamp=datetime.now(), ttl=ttl)

    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        now = datetime.now()
        valid_items = 0
        expired_items = 0

        for item in self._cache.values():
            if now - item.timestamp <= timedelta(seconds=item.ttl):
                valid_items += 1
            else:
                expired_items += 1

        return {
            "total_items": len(self._cache),
            "valid_items": valid_items,
            "expired_items": expired_items,
            "cache_hit_ratio": valid_items / max(len(self._cache), 1),
        }


# ================== äº‰è®®æ€§åˆ†æå™¨ ==================


class ControversyAnalyzer:
    """äº‰è®®æ€§åˆ†æå™¨"""
    
    def __init__(self):
        # äº‰è®®æ€§å…³é”®è¯æƒé‡
        self.controversy_keywords = {
            # é«˜äº‰è®®æ€§ (æƒé‡ 1.0)
            "äº‰è®®": 1.0, "æ‰¹è¯„": 1.0, "æŠ—è®®": 1.0, "åå¯¹": 1.0, "å†²çª": 1.0,
            "scandal": 1.0, "controversy": 1.0, "protest": 1.0, "crisis": 1.0,
            
            # ä¸­ç­‰äº‰è®®æ€§ (æƒé‡ 0.7)
            "è®¨è®º": 0.7, "è´¨ç–‘": 0.7, "åˆ†æ­§": 0.7, "è¾©è®º": 0.7, "çƒ­è®®": 0.7,
            "debate": 0.7, "discussion": 0.7, "question": 0.7, "concern": 0.7,
            
            # ä½äº‰è®®æ€§ (æƒé‡ 0.5)
            "å˜åŒ–": 0.5, "æ–°æ”¿": 0.5, "æ”¹é©": 0.5, "è°ƒæ•´": 0.5, "æ›´æ–°": 0.5,
            "change": 0.5, "reform": 0.5, "update": 0.5, "new": 0.5,
            
            # ç¤¾ä¼šè¯é¢˜ (æƒé‡ 0.8)
            "æˆ¿ä»·": 0.8, "æ•™è‚²": 0.8, "å°±ä¸š": 0.8, "åŒ»ç–—": 0.8, "å…»è€": 0.8,
            "housing": 0.8, "education": 0.8, "healthcare": 0.8, "employment": 0.8,
            
            # æƒ…æ„Ÿè¯æ±‡ (æƒé‡ 0.6)
            "æ„¤æ€’": 0.6, "ä¸æ»¡": 0.6, "æ‹…å¿ƒ": 0.6, "ç„¦è™‘": 0.6, "å¤±æœ›": 0.6,
            "angry": 0.6, "upset": 0.6, "worried": 0.6, "disappointed": 0.6
        }
    
    def calculate_controversy_score(self, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬çš„äº‰è®®æ€§åˆ†æ•°"""
        if not text:
            return 0.0
        
        text_lower = text.lower()
        score = 0.0
        word_count = 0
        
        for keyword, weight in self.controversy_keywords.items():
            if keyword in text_lower:
                score += weight
                word_count += 1
        
        # æ ‡å‡†åŒ–åˆ†æ•° (0-1ä¹‹é—´)
        if word_count > 0:
            score = min(score / word_count, 1.0)
        
        # å¦‚æœåŒ…å«é—®å·æˆ–æ„Ÿå¹å·ï¼Œå¢åŠ äº‰è®®æ€§
        if "?" in text or "ï¼Ÿ" in text:
            score += 0.2
        if "!" in text or "ï¼" in text:
            score += 0.1
        
        return min(score, 1.0)


# ================== æ•°æ®æä¾›è€… ==================


class HotNewsProvider:
    """çƒ­ç‚¹æ–°é—»æ•°æ®æä¾›è€…ï¼Œæ”¯æŒå›½å†…å’Œå…¨çƒå¹³å°"""

    def __init__(self):
        self.cache_manager = CacheManager()
        self.controversy_analyzer = ControversyAnalyzer()
        self.config = mcp_config  # æ·»åŠ é…ç½®å¼•ç”¨

        # vvhan APIåŸºç¡€URLï¼ˆå›½å†…å¹³å°ï¼‰
        self.vvhan_base_url = "https://api.vvhan.com/api/hotlist"

        # å›½å†…å¹³å°æ˜ å°„é…ç½®
        self.domestic_platforms = {
            "zhihu": {"name": "çŸ¥ä¹çƒ­æ¦œ", "api_path": "zhihuHot", "vvhan": True},
            "weibo": {"name": "å¾®åšçƒ­æœ", "api_path": "weibo", "vvhan": True},
            "baidu": {"name": "ç™¾åº¦çƒ­æœ", "api_path": "baiduRY", "vvhan": True},
            "bilibili": {"name": "Bç«™çƒ­é—¨", "api_path": "bili", "vvhan": True},
            "douyin": {"name": "æŠ–éŸ³çƒ­ç‚¹", "api_path": "douyinHot", "vvhan": True},
            "toutiao": {"name": "ä»Šæ—¥å¤´æ¡", "api_path": "toutiao", "vvhan": True},
            "hupu": {"name": "è™æ‰‘çƒ­å¸–", "api_path": "hupu", "vvhan": True},
            "douban": {"name": "è±†ç“£çƒ­é—¨", "api_path": "douban", "vvhan": True},
            "ithome": {"name": "ITä¹‹å®¶", "api_path": "ithome", "vvhan": True},
        }
        
        # å…¨çƒå¹³å°é…ç½®
        self.global_platforms = {
            "google_trends": {"name": "Google Trends", "type": "serpapi"},
            "news_api": {"name": "NewsAPI", "type": "newsapi"},
            "reddit": {"name": "Reddit", "type": "reddit"},
            "twitter": {"name": "Twitter/X", "type": "twitter"},
        }
        
        # åˆå¹¶æ‰€æœ‰å¹³å°
        self.platforms = {**self.domestic_platforms, **self.global_platforms}

    async def get_platform_news(
        self, platform: str, limit: int = 20
    ) -> Optional[PlatformNews]:
        """è·å–æŒ‡å®šå¹³å°çš„çƒ­ç‚¹æ–°é—»"""
        if platform not in self.platforms:
            logger.error(f"ä¸æ”¯æŒçš„å¹³å°: {platform}")
            return None

        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"news_{platform}_{limit}"
        cached_data = self.cache_manager.get(cache_key)
        if cached_data:
            logger.info(f"ä½¿ç”¨ç¼“å­˜æ•°æ®: {platform}")
            return cached_data

        platform_info = self.platforms[platform]
        platform_name = platform_info["name"]

        logger.info(f"è·å– {platform_name} çƒ­ç‚¹æ•°æ®...")

        try:
            # å›½å†…å¹³å°ä½¿ç”¨vvhan API
            if platform in self.domestic_platforms:
                return await self._get_domestic_platform_news(platform, limit)
            # å…¨çƒå¹³å°ä½¿ç”¨å„è‡ªçš„API
            elif platform in self.global_platforms:
                return await self._get_global_platform_news(platform, limit)
            else:
                return await self._get_mock_news(platform, limit)

        except Exception as e:
            logger.error(f"è·å– {platform_name} æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return await self._get_mock_news(platform, limit)

    async def _get_domestic_platform_news(self, platform: str, limit: int) -> Optional[PlatformNews]:
        """è·å–å›½å†…å¹³å°æ–°é—»ï¼ˆä½¿ç”¨vvhan APIï¼‰"""
        platform_info = self.domestic_platforms[platform]
        platform_name = platform_info["name"]
        
        if platform_info.get("vvhan", False):
            # ä½¿ç”¨vvhan API
            api_path = platform_info["api_path"]
            url = f"{self.vvhan_base_url}/{api_path}"

            async with httpx.AsyncClient(timeout=10.0) as client:
                try:
                    response = await client.get(url)

                    if response.status_code == 200:
                        data = response.json()

                        # æ£€æŸ¥APIæ˜¯å¦è¿”å›æˆåŠŸçŠ¶æ€
                        if data.get("success") is True and "data" in data:
                            # vvhan APIè¿”å›çš„æ•°æ®æ ¼å¼
                            raw_data = data["data"]

                            # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                            items = []
                            if isinstance(raw_data, dict) and "list" in raw_data:
                                # æ ¼å¼1: {"data": {"list": [...]}}
                                items = raw_data["list"][:limit]
                            elif isinstance(raw_data, list):
                                # æ ¼å¼2: {"data": [...]}
                                items = raw_data[:limit]
                            else:
                                logger.warning(f"vvhan APIè¿”å›æ„å¤–æ ¼å¼ - å¹³å°: {platform_name}, æ•°æ®ç±»å‹: {type(raw_data)}")
                                return await self._get_mock_news(platform, limit)

                            # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                            news_items = []
                            for i, item in enumerate(items):
                                if isinstance(item, dict):
                                    title = item.get("title", "")
                                    if title:  # åªå¤„ç†æœ‰æ ‡é¢˜çš„é¡¹ç›®
                                        # å¤„ç†ä¸åŒçš„å­—æ®µå
                                        url_field = item.get("url") or item.get("link") or item.get("mobil_url", "")
                                        hot_field = item.get("hot") or item.get("heat") or item.get("index", 0)
                                        
                                        news_item = NewsItem(
                                            title=title,
                                            url=url_field,
                                            hot_value=hot_field,
                                            rank=i + 1,
                                            platform=platform_name,
                                            timestamp=datetime.now().isoformat(),
                                            description=title[:100] if title else "",
                                            source=platform_name,
                                            controversy_score=self.controversy_analyzer.calculate_controversy_score(title),
                                            engagement_potential=0.7  # å›½å†…å¹³å°é»˜è®¤äº’åŠ¨æ½œåŠ›
                                        )
                                        news_items.append(news_item)

                            if news_items:  # åªæœ‰å½“æœ‰æœ‰æ•ˆæ•°æ®æ—¶æ‰è¿”å›
                                platform_news = PlatformNews(
                                    platform=platform_name,
                                    news_list=news_items,
                                    update_time=datetime.now().isoformat(),
                                    total_count=len(news_items),
                                    platform_type="domestic"
                                )

                                # ç¼“å­˜ç»“æœ
                                cache_key = f"news_{platform}_{limit}"
                                self.cache_manager.set(cache_key, platform_news)
                                logger.info(f"âœ… {platform_name} æ•°æ®è·å–æˆåŠŸ: {len(news_items)}æ¡")
                                return platform_news
                            else:
                                logger.warning(f"vvhan APIè¿”å›ç©ºæ•°æ® - å¹³å°: {platform_name}")
                                return await self._get_mock_news(platform, limit)
                        else:
                            # APIè¿”å›å¤±è´¥æˆ–æ ¼å¼ä¸æ­£ç¡®
                            success_status = data.get("success")
                            message = data.get("message", "æœªçŸ¥é”™è¯¯")
                            logger.warning(f"vvhan APIè¿”å›å¤±è´¥ - å¹³å°: {platform_name}, success: {success_status}, message: {message}")
                            return await self._get_mock_news(platform, limit)
                    else:
                        logger.warning(f"vvhan APIè¯·æ±‚å¤±è´¥ - å¹³å°: {platform_name}, çŠ¶æ€ç : {response.status_code}")
                        return await self._get_mock_news(platform, limit)
                        
                except Exception as e:
                    logger.error(f"è·å– {platform_name} æ•°æ®æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
                    return await self._get_mock_news(platform, limit)
        
        return await self._get_mock_news(platform, limit)

    async def _get_global_platform_news(self, platform: str, limit: int) -> Optional[PlatformNews]:
        """è·å–å…¨çƒå¹³å°æ–°é—»"""
        platform_info = self.global_platforms[platform]
        platform_type = platform_info["type"]
        
        if platform_type == "serpapi":
            return await self._get_google_trends_data(limit)
        elif platform_type == "newsapi":
            return await self._get_news_api_data(limit)
        elif platform_type == "reddit":
            return await self._get_reddit_data(limit)
        elif platform_type == "twitter":
            return await self._get_twitter_data(limit)
        else:
            return await self._get_mock_news(platform, limit)

    async def _get_google_trends_data(self, limit: int) -> Optional[PlatformNews]:
        """è·å–Google Trendsæ•°æ®"""
        try:
            serpapi_key = self.config.serpapi_key
            if not serpapi_key:
                logger.warning("SERPAPI_KEYæœªé…ç½®ï¼Œè·³è¿‡Google Trendsæ•°æ®è·å–")
                return await self._get_mock_news("google_trends", limit)
            
            # Google Trends Trending Now APIé€šè¿‡SerpAPI
            url = "https://serpapi.com/search"
            params = {
                "engine": "google_trends_trending_now",
                "geo": "US",  # ä½¿ç”¨æ­£ç¡®çš„geoå‚æ•°
                "hl": "en",   # è¯­è¨€
                "api_key": serpapi_key
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    news_items = []
                    
                    # è§£æGoogle Trendsæ•°æ®
                    trending_searches = data.get("trending_searches", [])
                    if trending_searches:
                        for i, item in enumerate(trending_searches[:limit]):
                            query = item.get("query", "")
                            if query:
                                news_item = NewsItem(
                                    title=query,
                                    url=f"https://trends.google.com/trends/explore?q={query}",
                                    hot_value=item.get('search_volume', 'N/A'),
                                    rank=i + 1,
                                    platform="Google Trends",
                                    timestamp=datetime.now().isoformat(),
                                    description=f"Googleæœç´¢çƒ­åº¦: {item.get('search_volume', 'N/A')}",
                                    source="Google Trends",
                                    controversy_score=self.controversy_analyzer.calculate_controversy_score(query),
                                    engagement_potential=0.8  # Googleè¶‹åŠ¿é€šå¸¸æœ‰é«˜äº’åŠ¨æ½œåŠ›
                                )
                                news_items.append(news_item)
                    
                    platform_news = PlatformNews(
                        platform="Google Trends",
                        news_list=news_items,
                        update_time=datetime.now().isoformat(),
                        total_count=len(news_items),
                        platform_type="global"
                    )
                    
                    cache_key = f"news_google_trends_{limit}"
                    self.cache_manager.set(cache_key, platform_news)
                    logger.info(f"âœ… Google Trends æ•°æ®è·å–æˆåŠŸ: {len(news_items)}æ¡")
                    return platform_news
                else:
                    logger.warning(f"Google Trends APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    return await self._get_mock_news("google_trends", limit)
                    
        except Exception as e:
            logger.error(f"è·å–Google Trendsæ•°æ®å¤±è´¥: {e}")
            return await self._get_mock_news("google_trends", limit)

    async def _get_news_api_data(self, limit: int) -> Optional[PlatformNews]:
        """è·å–NewsAPIæ•°æ®"""
        try:
            newsapi_key = self.config.newsapi_key
            if not newsapi_key:
                logger.warning("NEWSAPI_KEYæœªé…ç½®ï¼Œè·³è¿‡NewsAPIæ•°æ®è·å–")
                return await self._get_mock_news("news_api", limit)
            
            # NewsAPIçƒ­é—¨æ–°é—»
            url = "https://newsapi.org/v2/top-headlines"
            params = {
                "category": "general",
                "pageSize": min(limit, 20),  # NewsAPIé™åˆ¶
                "apiKey": newsapi_key,
                "language": "en"
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(url, params=params)
                if response.status_code == 200:
                    data = response.json()
                    news_items = []
                    
                    # è§£ææ–°é—»æ•°æ®
                    articles = data.get("articles", [])
                    for i, article in enumerate(articles):
                        title = article.get("title", "")
                        if title and title != "[Removed]" and len(title) > 10:
                            news_item = NewsItem(
                                title=title,
                                url=article.get("url", ""),
                                hot_value="N/A",
                                rank=i + 1,
                                platform="NewsAPI",
                                timestamp=datetime.now().isoformat(),
                                description=article.get("description", "")[:100] if article.get("description") else "",
                                source=f"NewsAPI - {article.get('source', {}).get('name', 'Unknown')}",
                                controversy_score=self.controversy_analyzer.calculate_controversy_score(title),
                                engagement_potential=0.7  # æ–°é—»é€šå¸¸æœ‰è¾ƒé«˜çš„äº’åŠ¨æ½œåŠ›
                            )
                            news_items.append(news_item)
                    
                    platform_news = PlatformNews(
                        platform="NewsAPI",
                        news_list=news_items,
                        update_time=datetime.now().isoformat(),
                        total_count=len(news_items),
                        platform_type="global"
                    )
                    
                    cache_key = f"news_news_api_{limit}"
                    self.cache_manager.set(cache_key, platform_news)
                    logger.info(f"âœ… NewsAPI æ•°æ®è·å–æˆåŠŸ: {len(news_items)}æ¡")
                    return platform_news
                else:
                    logger.warning(f"NewsAPIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    return await self._get_mock_news("news_api", limit)
                    
        except Exception as e:
            logger.error(f"è·å–NewsAPIæ•°æ®å¤±è´¥: {e}")
            return await self._get_mock_news("news_api", limit)

    async def _get_reddit_data(self, limit: int) -> Optional[PlatformNews]:
        """è·å–Redditæ•°æ®"""
        try:
            client_id = self.config.reddit_client_id
            client_secret = self.config.reddit_client_secret
            user_agent = self.config.reddit_user_agent
            
            if not self.config.has_reddit():
                logger.warning("Reddit APIé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡Redditæ•°æ®è·å–")
                return await self._get_mock_news("reddit", limit)
            
            # Reddit OAuthè®¤è¯
            auth_url = "https://www.reddit.com/api/v1/access_token"
            auth_data = {"grant_type": "client_credentials"}
            
            credentials = f"{client_id}:{client_secret}"
            encoded_credentials = base64.b64encode(credentials.encode()).decode()
            
            headers = {
                "Authorization": f"Basic {encoded_credentials}",
                "User-Agent": user_agent
            }
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                # è·å–è®¿é—®ä»¤ç‰Œ
                response = await client.post(auth_url, data=auth_data, headers=headers)
                if response.status_code == 200:
                    token_data = response.json()
                    access_token = token_data.get("access_token")
                    
                    if access_token:
                        # è·å–çƒ­é—¨å¸–å­
                        api_headers = {
                            "Authorization": f"Bearer {access_token}",
                            "User-Agent": user_agent
                        }
                        
                        news_items = []
                        subreddits = ["popular", "worldnews"]
                        
                        for subreddit in subreddits:
                            reddit_url = f"https://oauth.reddit.com/r/{subreddit}/hot"
                            params = {"limit": min(limit // len(subreddits), 10)}
                            
                            reddit_response = await client.get(reddit_url, headers=api_headers, params=params)
                            if reddit_response.status_code == 200:
                                reddit_data = reddit_response.json()
                                
                                for post in reddit_data.get("data", {}).get("children", []):
                                    post_data = post.get("data", {})
                                    title = post_data.get("title", "")
                                    
                                    if title and len(title) > 10:
                                        news_item = NewsItem(
                                            title=title,
                                            url=f"https://reddit.com{post_data.get('permalink', '')}",
                                            hot_value=post_data.get('score', 0),
                                            rank=len(news_items) + 1,
                                            platform="Reddit",
                                            timestamp=datetime.now().isoformat(),
                                            description=f"Reddit r/{subreddit} - {post_data.get('score', 0)} upvotes",
                                            source=f"Reddit r/{subreddit}",
                                            controversy_score=self.controversy_analyzer.calculate_controversy_score(title),
                                            engagement_potential=min(post_data.get('score', 0) / 1000.0, 1.0)
                                        )
                                        news_items.append(news_item)
                        
                        platform_news = PlatformNews(
                            platform="Reddit",
                            news_list=news_items[:limit],
                            update_time=datetime.now().isoformat(),
                            total_count=len(news_items[:limit]),
                            platform_type="global"
                        )
                        
                        cache_key = f"news_reddit_{limit}"
                        self.cache_manager.set(cache_key, platform_news)
                        logger.info(f"âœ… Reddit æ•°æ®è·å–æˆåŠŸ: {len(news_items[:limit])}æ¡")
                        return platform_news
                
                logger.warning(f"Redditè®¤è¯å¤±è´¥: {response.status_code}")
                return await self._get_mock_news("reddit", limit)
                
        except Exception as e:
            logger.error(f"è·å–Redditæ•°æ®å¤±è´¥: {e}")
            return await self._get_mock_news("reddit", limit)

    async def _get_twitter_data(self, limit: int) -> Optional[PlatformNews]:
        """è·å–Twitteræ•°æ®"""
        try:
            # å°è¯•ä½¿ç”¨ç¬¬ä¸‰æ–¹Twitter APIæœåŠ¡
            twitterapi_key = self.config.twitter_api_io_token
            zyla_key = self.config.zyla_api_key
            
            if twitterapi_key:
                return await self._get_twitter_via_twitterapi_io(twitterapi_key, limit)
            elif zyla_key:
                return await self._get_twitter_via_zyla(zyla_key, limit)
            else:
                logger.warning("Twitter APIé…ç½®ä¸å®Œæ•´ï¼Œè·³è¿‡Twitteræ•°æ®è·å–")
                return await self._get_mock_news("twitter", limit)
                
        except Exception as e:
            logger.error(f"è·å–Twitteræ•°æ®å¤±è´¥: {e}")
            return await self._get_mock_news("twitter", limit)

    async def _get_twitter_via_twitterapi_io(self, api_key: str, limit: int) -> Optional[PlatformNews]:
        """é€šè¿‡TwitterAPI.ioè·å–Twitterè¶‹åŠ¿"""
        try:
            url = "https://api.twitterapi.io/twitter/trend/trending"
            headers = {"X-API-Key": api_key}
            params = {"woeid": "1"}  # å…¨çƒè¶‹åŠ¿
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(url, headers=headers, params=params)
                if response.status_code == 200:
                    data = response.json()
                    news_items = []
                    
                    for i, trend in enumerate(data.get("trends", [])[:limit]):
                        title = trend.get("name", "")
                        if title:
                            news_item = NewsItem(
                                title=title,
                                url=trend.get("url", ""),
                                hot_value=trend.get('tweet_volume', 0),
                                rank=i + 1,
                                platform="Twitter",
                                timestamp=datetime.now().isoformat(),
                                description=f"Twitterè¶‹åŠ¿ - {trend.get('tweet_volume', 0)} tweets",
                                source="Twitter (TwitterAPI.io)",
                                controversy_score=self.controversy_analyzer.calculate_controversy_score(title),
                                engagement_potential=min((trend.get('tweet_volume', 0) or 0) / 10000.0, 1.0)
                            )
                            news_items.append(news_item)
                    
                    platform_news = PlatformNews(
                        platform="Twitter",
                        news_list=news_items,
                        update_time=datetime.now().isoformat(),
                        total_count=len(news_items),
                        platform_type="global"
                    )
                    
                    cache_key = f"news_twitter_{limit}"
                    self.cache_manager.set(cache_key, platform_news)
                    logger.info(f"âœ… Twitter æ•°æ®è·å–æˆåŠŸ: {len(news_items)}æ¡")
                    return platform_news
                else:
                    logger.warning(f"TwitterAPI.ioè¯·æ±‚å¤±è´¥: {response.status_code}")
                    return await self._get_mock_news("twitter", limit)
                    
        except Exception as e:
            logger.error(f"TwitterAPI.ioè·å–æ•°æ®å¤±è´¥: {e}")
            return await self._get_mock_news("twitter", limit)

    async def _get_twitter_via_zyla(self, api_key: str, limit: int) -> Optional[PlatformNews]:
        """é€šè¿‡Zyla APIè·å–Twitterè¶‹åŠ¿"""
        try:
            url = "https://zylalabs.com/api/1858/twitter+x+trends+api/5930/trends+capture"
            headers = {"Authorization": f"Bearer {api_key}"}
            params = {"woeid": "1"}  # å…¨çƒè¶‹åŠ¿
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(url, headers=headers, params=params)
                if response.status_code == 200:
                    data = response.json()
                    news_items = []
                    
                    for i, trend in enumerate(data.get("trends", [])[:limit]):
                        title = trend.get("name", "")
                        if title:
                            news_item = NewsItem(
                                title=title,
                                url=trend.get("url", ""),
                                hot_value="N/A",
                                rank=i + 1,
                                platform="Twitter",
                                timestamp=datetime.now().isoformat(),
                                description=f"Twitterè¶‹åŠ¿ - {trend.get('tweet_volume_text', 'æœªçŸ¥')} tweets",
                                source="Twitter (Zyla API)",
                                controversy_score=self.controversy_analyzer.calculate_controversy_score(title),
                                engagement_potential=0.8  # Twitterè¶‹åŠ¿é€šå¸¸æœ‰é«˜äº’åŠ¨æ½œåŠ›
                            )
                            news_items.append(news_item)
                    
                    platform_news = PlatformNews(
                        platform="Twitter",
                        news_list=news_items,
                        update_time=datetime.now().isoformat(),
                        total_count=len(news_items),
                        platform_type="global"
                    )
                    
                    cache_key = f"news_twitter_{limit}"
                    self.cache_manager.set(cache_key, platform_news)
                    logger.info(f"âœ… Twitter æ•°æ®è·å–æˆåŠŸ: {len(news_items)}æ¡")
                    return platform_news
                else:
                    logger.warning(f"Zyla APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    return await self._get_mock_news("twitter", limit)
                    
        except Exception as e:
            logger.error(f"Zyla APIè·å–æ•°æ®å¤±è´¥: {e}")
            return await self._get_mock_news("twitter", limit)

    async def get_all_platforms_news(self, limit: int = 10) -> List[PlatformNews]:
        """è·å–æ‰€æœ‰å¹³å°çƒ­ç‚¹æ–°é—»"""
        all_news = []

        tasks = [
            self.get_platform_news(platform, limit)
            for platform in self.platforms.keys()
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, PlatformNews):
                all_news.append(result)
            elif isinstance(result, Exception):
                logger.error(f"è·å–å¹³å°æ–°é—»å¤±è´¥: {result}")

        return all_news

    async def get_domestic_platforms_news(self, limit: int = 10) -> List[PlatformNews]:
        """è·å–å›½å†…å¹³å°çƒ­ç‚¹æ–°é—»"""
        all_news = []

        tasks = [
            self.get_platform_news(platform, limit)
            for platform in self.domestic_platforms.keys()
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, PlatformNews):
                all_news.append(result)
            elif isinstance(result, Exception):
                logger.error(f"è·å–å›½å†…å¹³å°æ–°é—»å¤±è´¥: {result}")

        return all_news

    async def get_global_platforms_news(self, limit: int = 10) -> List[PlatformNews]:
        """è·å–å…¨çƒå¹³å°çƒ­ç‚¹æ–°é—»"""
        all_news = []

        tasks = [
            self.get_platform_news(platform, limit)
            for platform in self.global_platforms.keys()
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        for result in results:
            if isinstance(result, PlatformNews):
                all_news.append(result)
            elif isinstance(result, Exception):
                logger.error(f"è·å–å…¨çƒå¹³å°æ–°é—»å¤±è´¥: {result}")

        return all_news

    async def _get_mock_news(self, platform: str, limit: int) -> PlatformNews:
        """é™çº§åˆ°æ¨¡æ‹Ÿæ•°æ®"""
        platform_info = self.platforms.get(platform, {"name": platform})
        platform_name = platform_info["name"]

        logger.info(f"ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ® - å¹³å°: {platform_name}")

        # æ ¹æ®å¹³å°ç”Ÿæˆä¸åŒç±»å‹çš„æ¨¡æ‹Ÿæ•°æ®
        base_titles = {
            "zhihu": [
                "çŸ¥ä¹çƒ­é—¨è¯é¢˜",
                "æ·±åº¦æ€è€ƒé—®é¢˜",
                "ä¸“ä¸šé¢†åŸŸè®¨è®º",
                "ç”Ÿæ´»ç»éªŒåˆ†äº«",
                "ç§‘æŠ€å‰æ²¿æ¢è®¨",
            ],
            "weibo": [
                "å¾®åšçƒ­æœè¯é¢˜",
                "å¨±ä¹æ˜æ˜ŸåŠ¨æ€",
                "ç¤¾ä¼šçƒ­ç‚¹äº‹ä»¶",
                "ç½‘ç»œæµè¡Œè¯é¢˜",
                "çªå‘æ–°é—»",
            ],
            "baidu": [
                "ç™¾åº¦çƒ­æœå…³é”®è¯",
                "æœç´¢çƒ­é—¨è¯æ±‡",
                "ç½‘æ°‘å…³æ³¨ç„¦ç‚¹",
                "çƒ­é—¨æœç´¢è¯",
                "æµè¡Œæœç´¢",
            ],
            "bilibili": ["Bç«™çƒ­é—¨è§†é¢‘", "UPä¸»åˆ›ä½œ", "åŠ¨æ¼«ç•ªå‰§", "æ¸¸æˆè§£è¯´", "çŸ¥è¯†ç§‘æ™®"],
            "douyin": [
                "æŠ–éŸ³çƒ­é—¨è¯é¢˜",
                "çŸ­è§†é¢‘è¯é¢˜",
                "åˆ›æ„æŒ‘æˆ˜",
                "éŸ³ä¹çƒ­é—¨",
                "ç”Ÿæ´»è®°å½•",
            ],
            "toutiao": ["ä»Šæ—¥å¤´æ¡æ–°é—»", "æ—¶äº‹è¦é—»", "ç¤¾ä¼šæ–°é—»", "ç§‘æŠ€èµ„è®¯", "è´¢ç»åŠ¨æ€"],
            "hupu": ["è™æ‰‘çƒ­å¸–", "ä½“è‚²è®¨è®º", "NBAè¯é¢˜", "è¶³çƒåˆ†æ", "è¿åŠ¨å¥èº«"],
            "douban": ["è±†ç“£çƒ­é—¨", "ç”µå½±è¯„è®º", "ä¹¦ç±æ¨è", "ç”Ÿæ´»å°ç»„", "æ–‡è‰ºè¯é¢˜"],
            "ithome": ["ITä¹‹å®¶", "ç§‘æŠ€æ–°é—»", "æ•°ç äº§å“", "è½¯ä»¶æ›´æ–°", "è¡Œä¸šåŠ¨æ€"],
            "google_trends": ["Googleçƒ­æœ", "å…¨çƒè¶‹åŠ¿", "æœç´¢çƒ­è¯", "æµè¡Œè¯é¢˜", "å›½é™…å…³æ³¨"],
            "news_api": ["å›½é™…æ–°é—»", "å…¨çƒèµ„è®¯", "çªå‘äº‹ä»¶", "æ”¿æ²»ç»æµ", "ç§‘æŠ€åˆ›æ–°"],
            "reddit": ["Redditçƒ­å¸–", "ç¤¾åŒºè®¨è®º", "å›½é™…è¯é¢˜", "æŠ€æœ¯åˆ†äº«", "æ–‡åŒ–äº¤æµ"],
            "twitter": ["Twitterè¶‹åŠ¿", "å…¨çƒçƒ­è®®", "å®æ—¶è¯é¢˜", "ç¤¾äº¤åª’ä½“", "å›½é™…åŠ¨æ€"],
        }

        titles = base_titles.get(platform, ["çƒ­é—¨è¯é¢˜", "æµè¡Œå†…å®¹", "ç”¨æˆ·å…³æ³¨", "çƒ­ç‚¹è®¨è®º", "è¶‹åŠ¿è¯é¢˜"])

        # ç”Ÿæˆæ¨¡æ‹Ÿæ–°é—»æ¡ç›®
        news_items = []
        for i in range(min(limit, len(titles))):
            title = f"{titles[i % len(titles)]} {i+1}"
            news_item = NewsItem(
                title=title,
                url=f"https://example.com/{platform}/{i+1}",
                hot_value=1000 - i * 50,
                rank=i + 1,
                platform=platform_name,
                timestamp=datetime.now().isoformat(),
                description=f"è¿™æ˜¯{platform_name}çš„æ¨¡æ‹Ÿçƒ­ç‚¹å†…å®¹",
                source=f"{platform_name} (æ¨¡æ‹Ÿæ•°æ®)",
                controversy_score=self.controversy_analyzer.calculate_controversy_score(title),
                engagement_potential=0.5  # æ¨¡æ‹Ÿæ•°æ®é»˜è®¤äº’åŠ¨æ½œåŠ›
            )
            news_items.append(news_item)

        # ç¡®å®šå¹³å°ç±»å‹
        platform_type = "global" if platform in self.global_platforms else "domestic"

        return PlatformNews(
            platform=platform_name,
            news_list=news_items,
            update_time=datetime.now().isoformat(),
            total_count=len(news_items),
            platform_type=platform_type
        )

    def analyze_trends(self, all_news: List[PlatformNews]) -> TrendAnalysis:
        """åˆ†æçƒ­ç‚¹è¶‹åŠ¿"""
        hot_keywords = []
        trending_topics = []
        platform_summary = {}
        controversy_scores = []

        for platform_news in all_news:
            platform_summary[platform_news.platform] = platform_news.total_count
            
            for news_item in platform_news.news_list:
                # æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
                title_words = news_item.title.split()
                hot_keywords.extend([word for word in title_words if len(word) > 2])
                
                # æ·»åŠ è¶‹åŠ¿è¯é¢˜
                trending_topics.append(news_item.title)
                
                # æ”¶é›†äº‰è®®æ€§åˆ†æ•°
                if news_item.controversy_score:
                    controversy_scores.append(news_item.controversy_score)

        # ç»Ÿè®¡æœ€çƒ­é—¨çš„å…³é”®è¯
        from collections import Counter
        keyword_counts = Counter(hot_keywords)
        hot_keywords = [word for word, count in keyword_counts.most_common(10)]
        
        # äº‰è®®æ€§åˆ†æ
        controversy_analysis = {
            "average_controversy": sum(controversy_scores) / len(controversy_scores) if controversy_scores else 0,
            "high_controversy_count": len([s for s in controversy_scores if s > 0.7]),
            "medium_controversy_count": len([s for s in controversy_scores if 0.3 < s <= 0.7]),
            "low_controversy_count": len([s for s in controversy_scores if s <= 0.3]),
        }

        return TrendAnalysis(
            hot_keywords=hot_keywords,
            trending_topics=trending_topics[:20],  # å–å‰20ä¸ªè¯é¢˜
            platform_summary=platform_summary,
            analysis_time=datetime.now().isoformat(),
            controversy_analysis=controversy_analysis
        )


# ================== FastMCP æœåŠ¡å™¨ ==================

# åˆ›å»ºFastMCPæœåŠ¡å™¨å®ä¾‹
mcp = FastMCP(
    name="HotNewsServer",
    instructions="""
    è¿™æ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„çƒ­ç‚¹æ–°é—»MCPæœåŠ¡å™¨ï¼Œæä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š
    1. è·å–æŒ‡å®šå¹³å°çƒ­ç‚¹æ–°é—»
    2. è·å–æ‰€æœ‰å¹³å°çƒ­ç‚¹æ–°é—»æ±‡æ€»
    3. åˆ†æçƒ­ç‚¹è¶‹åŠ¿å’Œå…³é”®è¯

    æ”¯æŒçš„å¹³å°åŒ…æ‹¬ï¼šå¾®åšã€çŸ¥ä¹ã€ç™¾åº¦ã€å“”å“©å“”å“©ã€æŠ–éŸ³ã€å¿«æ‰‹ã€ä»Šæ—¥å¤´æ¡ã€æ¾æ¹ƒæ–°é—»ã€ç½‘æ˜“æ–°é—»ã€é›ªçƒç­‰ã€‚
    æ‰€æœ‰æ•°æ®éƒ½æœ‰æ™ºèƒ½ç¼“å­˜æœºåˆ¶ï¼Œæé«˜å“åº”é€Ÿåº¦ã€‚
    """,
)

# åˆ›å»ºæ•°æ®æä¾›è€…å®ä¾‹
news_provider = HotNewsProvider()

# ================== MCP å·¥å…·å®šä¹‰ ==================


@mcp.tool()
async def get_hot_news(
    platform: str = Field(description="å¹³å°åç§°ï¼Œå¦‚ weibo, zhihu, baidu, google_trends, news_api, reddit, twitter ç­‰"),
    limit: int = Field(default=20, description="è·å–æ–°é—»æ•°é‡ï¼Œé»˜è®¤20æ¡"),
) -> str:
    """è·å–æŒ‡å®šå¹³å°çš„çƒ­ç‚¹æ–°é—»"""
    try:
        platform_news = await news_provider.get_platform_news(platform, limit)
        if platform_news:
            return json.dumps(
                platform_news.model_dump(), ensure_ascii=False, indent=2
            )
        else:
            return json.dumps(
                {"error": f"æ— æ³•è·å–å¹³å° {platform} çš„æ–°é—»æ•°æ®"}, ensure_ascii=False
            )
    except Exception as e:
        logger.error(f"è·å–çƒ­ç‚¹æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def get_all_platforms_news(
    limit: int = Field(default=10, description="æ¯ä¸ªå¹³å°è·å–æ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡")
) -> str:
    """è·å–æ‰€æœ‰å¹³å°ï¼ˆå›½å†…+å…¨çƒï¼‰çƒ­ç‚¹æ–°é—»"""
    try:
        all_news = await news_provider.get_all_platforms_news(limit)
        
        result = {
            "platforms": [news.model_dump() for news in all_news],
            "total_platforms": len(all_news),
            "update_time": datetime.now().isoformat(),
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"è·å–æ‰€æœ‰å¹³å°æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def get_domestic_platforms_news(
    limit: int = Field(default=10, description="æ¯ä¸ªå¹³å°è·å–æ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡")
) -> str:
    """è·å–å›½å†…å¹³å°çƒ­ç‚¹æ–°é—»"""
    try:
        domestic_news = await news_provider.get_domestic_platforms_news(limit)
        
        result = {
            "platforms": [news.model_dump() for news in domestic_news],
            "total_platforms": len(domestic_news),
            "platform_type": "domestic",
            "update_time": datetime.now().isoformat(),
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"è·å–å›½å†…å¹³å°æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def get_global_platforms_news(
    limit: int = Field(default=10, description="æ¯ä¸ªå¹³å°è·å–æ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡")
) -> str:
    """è·å–å…¨çƒå¹³å°çƒ­ç‚¹æ–°é—»"""
    try:
        global_news = await news_provider.get_global_platforms_news(limit)
        
        result = {
            "platforms": [news.model_dump() for news in global_news],
            "total_platforms": len(global_news),
            "platform_type": "global",
            "update_time": datetime.now().isoformat(),
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"è·å–å…¨çƒå¹³å°æ–°é—»æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def analyze_trends(
    limit: int = Field(default=10, description="æ¯ä¸ªå¹³å°åˆ†ææ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡")
) -> str:
    """åˆ†æçƒ­ç‚¹è¶‹åŠ¿"""
    try:
        all_news = await news_provider.get_all_platforms_news(limit)
        analysis = news_provider.analyze_trends(all_news)
        
        return json.dumps(analysis.model_dump(), ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"åˆ†æè¶‹åŠ¿æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def analyze_controversy_trends(
    limit: int = Field(default=10, description="æ¯ä¸ªå¹³å°åˆ†ææ–°é—»æ•°é‡ï¼Œé»˜è®¤10æ¡")
) -> str:
    """åˆ†æäº‰è®®æ€§è¶‹åŠ¿"""
    try:
        all_news = await news_provider.get_all_platforms_news(limit)
        
        # æ”¶é›†æ‰€æœ‰æ–°é—»é¡¹ç›®å¹¶æŒ‰äº‰è®®æ€§æ’åº
        all_items = []
        for platform_news in all_news:
            for item in platform_news.news_list:
                all_items.append({
                    "title": item.title,
                    "platform": item.platform,
                    "controversy_score": item.controversy_score,
                    "engagement_potential": item.engagement_potential,
                    "url": item.url
                })
        
        # æŒ‰äº‰è®®æ€§åˆ†æ•°æ’åº
        all_items.sort(key=lambda x: x["controversy_score"], reverse=True)
        
        result = {
            "high_controversy_items": all_items[:10],  # å‰10ä¸ªæœ€å…·äº‰è®®æ€§çš„
            "total_analyzed": len(all_items),
            "average_controversy": sum(item["controversy_score"] for item in all_items) / len(all_items) if all_items else 0,
            "analysis_time": datetime.now().isoformat()
        }
        
        return json.dumps(result, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"åˆ†æäº‰è®®æ€§è¶‹åŠ¿æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def get_server_health() -> str:
    """è·å–æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
    try:
        cache_stats = news_provider.cache_manager.get_stats()
        
        health_info = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "cache_stats": cache_stats,
            "supported_platforms": {
                "domestic": list(news_provider.domestic_platforms.keys()),
                "global": list(news_provider.global_platforms.keys()),
                "total": len(news_provider.platforms)
            },
            "api_status": {
                "serpapi_configured": bool(os.getenv('SERPAPI_KEY')),
                "newsapi_configured": bool(os.getenv('NEWSAPI_KEY')),
                "reddit_configured": bool(os.getenv('REDDIT_CLIENT_ID') and os.getenv('REDDIT_CLIENT_SECRET')),
                "twitter_configured": bool(os.getenv('TWITTERAPI_IO_KEY') or os.getenv('ZYLA_API_KEY'))
            }
        }
        
        return json.dumps(health_info, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"è·å–æœåŠ¡å™¨å¥åº·çŠ¶æ€æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"status": "error", "error": str(e)}, ensure_ascii=False)


@mcp.tool()
async def clear_cache() -> str:
    """æ¸…ç©ºç¼“å­˜"""
    try:
        news_provider.cache_manager.clear()
        return json.dumps(
            {
                "status": "success",
                "message": "ç¼“å­˜å·²æ¸…ç©º",
                "timestamp": datetime.now().isoformat(),
            },
            ensure_ascii=False,
        )
    except Exception as e:
        logger.error(f"æ¸…ç©ºç¼“å­˜æ—¶å‡ºé”™: {str(e)}")
        return json.dumps({"status": "error", "error": str(e)}, ensure_ascii=False)


# ================== èµ„æºå®šä¹‰ ==================


@mcp.resource("hot-news://platforms")
def get_supported_platforms() -> str:
    """è·å–æ”¯æŒçš„å¹³å°åˆ—è¡¨"""
    platforms_info = {
        "domestic_platforms": {
            platform: info["name"] for platform, info in news_provider.domestic_platforms.items()
        },
        "global_platforms": {
            platform: info["name"] for platform, info in news_provider.global_platforms.items()
        },
        "total_count": len(news_provider.platforms)
    }
    return json.dumps(platforms_info, ensure_ascii=False, indent=2)


@mcp.resource("hot-news://config")
def get_server_config() -> str:
    """è·å–æœåŠ¡å™¨é…ç½®ä¿¡æ¯"""
    config_info = {
        "server_name": "HotNewsServer",
        "version": "2.0.0",
        "description": "æ”¯æŒå›½å†…å’Œå…¨çƒå¹³å°çš„çƒ­ç‚¹æ–°é—»èšåˆæœåŠ¡",
        "features": [
            "å¤šå¹³å°çƒ­ç‚¹æ–°é—»èšåˆ",
            "æ™ºèƒ½ç¼“å­˜ç®¡ç†",
            "è¶‹åŠ¿åˆ†æ",
            "äº‰è®®æ€§åˆ†æ",
            "å›½å†…å¤–å¹³å°åˆ†ç±»",
            "å®æ—¶æ•°æ®è·å–"
        ],
        "supported_apis": [
            "vvhan API (å›½å†…å¹³å°)",
            "Google Trends (SerpAPI)",
            "NewsAPI",
            "Reddit API",
            "Twitter API (ç¬¬ä¸‰æ–¹)"
        ]
    }
    return json.dumps(config_info, ensure_ascii=False, indent=2)


# ================== ä¸»ç¨‹åºå…¥å£ ==================


def main():
    """å¯åŠ¨MCPæœåŠ¡å™¨"""
    logger.info("ğŸš€ å¯åŠ¨çƒ­ç‚¹æ–°é—»MCPæœåŠ¡å™¨...")
    logger.info(f"ğŸ“Š æ”¯æŒå¹³å°æ•°é‡: {len(news_provider.platforms)}")
    logger.info(f"ğŸ  å›½å†…å¹³å°: {list(news_provider.domestic_platforms.keys())}")
    logger.info(f"ğŸŒ å…¨çƒå¹³å°: {list(news_provider.global_platforms.keys())}")
    
    # æ£€æŸ¥APIé…ç½®
    api_status = []
    if os.getenv('SERPAPI_KEY'):
        api_status.append("âœ… Google Trends (SerpAPI)")
    else:
        api_status.append("âŒ Google Trends (SerpAPI) - æœªé…ç½®SERPAPI_KEY")
    
    if os.getenv('NEWSAPI_KEY'):
        api_status.append("âœ… NewsAPI")
    else:
        api_status.append("âŒ NewsAPI - æœªé…ç½®NEWSAPI_KEY")
    
    if os.getenv('REDDIT_CLIENT_ID') and os.getenv('REDDIT_CLIENT_SECRET'):
        api_status.append("âœ… Reddit API")
    else:
        api_status.append("âŒ Reddit API - æœªé…ç½®Redditå‡­æ®")
    
    if os.getenv('TWITTERAPI_IO_KEY') or os.getenv('ZYLA_API_KEY'):
        api_status.append("âœ… Twitter API")
    else:
        api_status.append("âŒ Twitter API - æœªé…ç½®Twitter APIå¯†é’¥")
    
    logger.info("ğŸ”§ APIé…ç½®çŠ¶æ€:")
    for status in api_status:
        logger.info(f"   {status}")
    
    mcp.run()


if __name__ == "__main__":
    main()
